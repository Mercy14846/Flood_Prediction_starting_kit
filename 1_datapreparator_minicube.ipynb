{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cv2 as cv\n",
    "import os\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import src.Utils as utils\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building of mini datacubes and preprocessing\n",
    "\n",
    "Full execution of this notebook on our server takes about 10mins\n",
    "\n",
    "This notebook focuses on three objectives :\n",
    "* The construction of a set of mini datacubes. Each mini datacube is a subset of the full resolution data aligned with each spatial ERA5 pixel (tile of about 31 sqkm)\n",
    "* Some data preprocessing : missing values imputation and filtering of out of france data. **The model will only be evaluated on french data**\n",
    "* Creation of two new static features (distance to surface_water and distance to sea)\n",
    "\n",
    "#### Creation of the paths \n",
    "If you modify *save_path* make sure to modify it accordingly in the *Baseline_model_01* notebook and *Baseline_model_02* notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"localdata\"\n",
    "isExist = os.path.exists(save_path)\n",
    "if not isExist:\n",
    "    os.makedirs(save_path)\n",
    "isExist = os.path.exists(save_path+\"/smallbox/label/\")\n",
    "if not isExist:\n",
    "    os.makedirs(save_path+\"/smallbox/label/\")\n",
    "isExist = os.path.exists(save_path+\"/smallbox/static/\")\n",
    "if not isExist:\n",
    "    os.makedirs(save_path+\"/smallbox/static/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1Sg1BL18GpRjN-SX6a65y9llv8mWBktW5'\n",
    "destination = save_path+'/training_data.zip' \n",
    "download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.usercontent.google.com/download?id=1Sg1BL18GpRjN-SX6a65y9llv8mWBktW5&confirm=t'\n",
    "output = save_path+'/training_data.zip' \n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"localdata/training_data.zip\", 'r') as zObject: \n",
    "    zObject.extractall( \n",
    "        path=\"localdata/raw/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5 = xr.open_dataset( save_path+'/raw/ERA5_train.nc') #Dynamic data at the 0.25 degree / ERA5 resolution\n",
    "static = xr.open_dataset(save_path+\"/raw/static_train.nc\") #Dynamic data at the 0.25 degree / ERA5 resolution\n",
    "final_label_ERA5 = xr.open_dataset(save_path+\"/raw/final_label_ERA5_train.nc\") #Dynamic data at the 0.25 degree / ERA5 resolution\n",
    "final_label = xr.open_dataset(save_path+\"/raw/final_label_train.nc\") #Dynamic data at the 0.25 degree / ERA5 resolution\n",
    "relevent_data_tag_binary_mask = xr.open_dataset(save_path+\"/raw/relevent_data_tag_binary_mask.nc\") # Mapping of permanent water and out of france data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERA5_train dataset contains two spatiotemporal features, computated dayly from 2000-01-01 up to 2004-01-01 at the 0.25 degree / ERA5 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static dataset contains 13 spatial static features at the 250m / Full resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_label_ERA5_train contains floods events at the ERA5 resolution.\n",
    "1 indicate sptial are where flood occured, 0 indicate spatial are without flood, nan indicates permanent water and out of france data. \n",
    "Warning : time dimension is define by the starting day of the flood and the ending day of the flood, days without floods are not in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label_ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full resolution Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion to float32\n",
    "\n",
    "To reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static['__xarray_dataarray_variable__'].values = static['__xarray_dataarray_variable__'].values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to sea feature computation\n",
    "\n",
    "Creation of *dist_sea* (distance to sea) feature from land sea limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static.sel(band=\"land_sea_limit\").__xarray_dataarray_variable__.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_limit = static[\"__xarray_dataarray_variable__\"].sel(band=\"land_sea_limit\")\n",
    "\n",
    "surface_waters_mask = sea_limit < 1\n",
    "\n",
    "surface_waters_mask_uint8 = surface_waters_mask.astype('uint8') * 255\n",
    "distance_to_sea = cv.distanceTransform(surface_waters_mask_uint8.values, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "\n",
    "distance_to_sea_da = xr.DataArray(distance_to_sea,\n",
    "                                    dims=['y', 'x'],  \n",
    "                                    coords={'y': static.coords['y'], 'x': static.coords['x']})\n",
    "\n",
    "distance_to_sea_da = distance_to_sea_da.expand_dims({'band': ['dist_sea']})\n",
    "\n",
    "\n",
    "combined_data = xr.concat([static['__xarray_dataarray_variable__'], distance_to_sea_da], dim='band')\n",
    "\n",
    "static_with_dts = xr.Dataset({'__xarray_dataarray_variable__': combined_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_with_dts[\"__xarray_dataarray_variable__\"].sel(band=\"dist_sea\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to surface_water feature computation\n",
    "\n",
    "Creation of *dist_water* (distance to surface_water) feature from water density (water density doesn't  includes only surface_waters but also any type of watercourse and lake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = static_with_dts[\"__xarray_dataarray_variable__\"].sel(band=\"water_density\")\n",
    "surface_waters_mask = temp < 25 # 25 is the threshold for water density, we compute water distance to  wit\n",
    "\n",
    "surface_waters_mask_uint8 = surface_waters_mask.astype('uint8') * 255\n",
    "distance_to_surface_water = cv.distanceTransform(surface_waters_mask_uint8.values, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "\n",
    "distance_to_surface_water_da = xr.DataArray(distance_to_surface_water,\n",
    "                                    dims=['y', 'x'],  \n",
    "                                    coords={'y': static_with_dts.coords['y'], 'x': static_with_dts.coords['x']})\n",
    "\n",
    "distance_to_surface_water_da = distance_to_surface_water_da.expand_dims({'band': ['dist_water']})\n",
    "\n",
    "\n",
    "combined_data = xr.concat([static_with_dts['__xarray_dataarray_variable__'], distance_to_surface_water_da], dim='band')\n",
    "\n",
    "static_with_dts_dtr = xr.Dataset({'__xarray_dataarray_variable__': combined_data})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering\n",
    "\n",
    "Label is set to -1 for data outside of France (using \"bassin_versants_cdbh\" data for filtering). The model will only be evaluated on France teritory data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bassin_versants_cdbh_band = static_with_dts_dtr[\"__xarray_dataarray_variable__\"].sel(band=\"watershed\")\n",
    "\n",
    "nan_mask = bassin_versants_cdbh_band == 0\n",
    "data_array = final_label[\"__xarray_dataarray_variable__\"]\n",
    "\n",
    "for time in final_label.time.values:\n",
    "    time_slice = data_array.sel(time=time)\n",
    "    time_slice = time_slice.where(~nan_mask, -1)    \n",
    "    data_array.loc[dict(time=time)] = time_slice\n",
    "\n",
    "data_array = data_array.fillna(-1)\n",
    "final_label[\"__xarray_dataarray_variable__\"] = data_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label.to_netcdf(f'{save_path}/final_label_Full_Rez.nc', engine='h5netcdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data imputation\n",
    "\n",
    "We use mean imputation as a naive imputation strategy for the baseline model. Other methods can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = static_with_dts_dtr[\"__xarray_dataarray_variable__\"]\n",
    "\n",
    "band_means = data_array.mean(dim=['x', 'y'], skipna=True)\n",
    "\n",
    "for band in static_with_dts_dtr.band.values:  \n",
    "    band_mean = band_means.sel(band=band).item()\n",
    "    updated_band = data_array.sel(band=band).fillna(band_mean)\n",
    "    data_array.loc[dict(band=band)] = updated_band\n",
    "\n",
    "static_with_dts_dtr[\"__xarray_dataarray_variable__\"] = data_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of land_sea_limit\n",
    "\n",
    "Land_sea_limit is used to comptute *dist_sea* feature but does not seems in itself a useful feature, we choose to remove it from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bands = static_with_dts_dtr.coords['band'].values\n",
    "\n",
    "bands_except_dist_water = [band for band in all_bands if band != \"land_sea_limit\"]\n",
    "\n",
    "selected_bands = static_with_dts_dtr.sel(band=bands_except_dist_water)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving of Full Rez static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {'__xarray_dataarray_variable__': {'dtype': 'float32'}}\n",
    "selected_bands.to_netcdf(f'{save_path}/static_Full_Rez.nc', encoding=encoding, engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_static = xr.open_dataset(f'{save_path}/static_Full_Rez.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5 Resolution Data processing\n",
    "\n",
    "To train the first model, we need to chunk the geospatial data and the labels at the ERA5 spatial resolution. The chunked labels for flood maps are already provided, we just need to create the complete label set over the time period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation of the static data at the ERA5 spatial resolution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages(big_ds, small_ds):\n",
    "    averaged_values = {}\n",
    "\n",
    "    for band in big_ds.band.values:\n",
    "        band_averaged_values = np.zeros((len(small_ds.y), len(small_ds.x)), dtype=np.float32)\n",
    "\n",
    "        for i, y in enumerate(small_ds.y.values):\n",
    "            for j, x in enumerate(small_ds.x.values):\n",
    "                corresponding_pixels = big_ds.sel(x=x, y=y, method=\"nearest\").sel(band=band).__xarray_dataarray_variable__.values\n",
    "                band_averaged_values[i, j] = np.nanmean(corresponding_pixels)  \n",
    "\n",
    "        averaged_values[str(band)] = band_averaged_values\n",
    "\n",
    "    return averaged_values\n",
    "\n",
    "averaged_data_static = compute_averages(selected_bands, final_label_ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ERA5_output = xr.Dataset(\n",
    "    {band: ([\"y\", \"x\"], averaged_data_static[band]) for band in averaged_data_static},\n",
    "    coords={\n",
    "        \"x\": final_label_ERA5.x.values,\n",
    "        \"y\": final_label_ERA5.y.values\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Geospatial data at EAR5 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_ERA5_output.to_netcdf(f'{save_path}/static_ERA5.nc', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the ERA5 resolution labels from flood maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering of out of France data on the ERA5 resolution label, we use watershed feature to remove data outside of france from the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_band = averaged_data_static[\"watershed\"]\n",
    "\n",
    "nan_mask = watershed_band == 0\n",
    "data_array = final_label_ERA5[\"__xarray_dataarray_variable__\"]\n",
    "\n",
    "for time in final_label_ERA5.band.values:\n",
    "    time_slice = data_array.sel(band=time)\n",
    "    time_slice = time_slice.where(~nan_mask, -1)    \n",
    "    data_array.loc[dict(band=time)] = time_slice\n",
    "\n",
    "final_label_ERA5[\"__xarray_dataarray_variable__\"] = data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spread the flood maps over the standardise weekly time dimension and add empty maps for non flooded weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = final_label_ERA5.where(final_label_ERA5.band != '20030101-200301080000000000-0000014848', drop=True)\n",
    "# renanming of the duplicated band/date\n",
    "band_series = pd.Series(raw_labels.band.values)\n",
    "band_series = band_series.replace({'20030101-200301080000000000-0000000000': '20030101-20030108'})\n",
    "raw_labels = raw_labels.assign_coords(band=band_series.values)\n",
    "\n",
    "# convert the band/date to datetime64\n",
    "original_band_dates = [utils.parse_dates(band) for band in raw_labels.band.values]\n",
    "\n",
    "# filter the bands/dates to keep only the ones between start_date and end_date\n",
    "start_date = pd.to_datetime(\"2002-08-03\")\n",
    "end_date = pd.to_datetime(\"2004-01-01\")\n",
    "\n",
    "# addition of the missing bands/dates (weeks without flood)\n",
    "new_bands = pd.date_range(start=start_date, end=end_date, freq='W').values        \n",
    "new_data = np.full((len(new_bands), *raw_labels.__xarray_dataarray_variable__.shape[1:]), np.nan)\n",
    "#filling the new_data with the original data (with flood)\n",
    "outside_of_france_mask = raw_labels['__xarray_dataarray_variable__'][0]==-1\n",
    "\n",
    "for i, new_band in enumerate(new_bands):\n",
    "    new_data[i][outside_of_france_mask] = -1\n",
    "    for old_start, old_end in original_band_dates:\n",
    "        if (new_band >= np.datetime64(old_start)) and (new_band <= np.datetime64(old_end)):\n",
    "            old_band_index = np.where(raw_labels.band == f'{old_start.strftime(\"%Y%m%d\")}-{old_end.strftime(\"%Y%m%d\")}')[0][0]\n",
    "            new_data[i] = raw_labels.__xarray_dataarray_variable__[old_band_index].values\n",
    "        elif (new_band <= np.datetime64(old_end)) and (new_band >= (np.datetime64(old_start)- np.timedelta64(7,'D'))):\n",
    "            old_band_index = np.where(raw_labels.band == f'{old_start.strftime(\"%Y%m%d\")}-{old_end.strftime(\"%Y%m%d\")}')[0][0]\n",
    "            new_data[i] = raw_labels.__xarray_dataarray_variable__[old_band_index].values\n",
    "\n",
    "\n",
    "new_da = xr.DataArray(new_data, coords=[new_bands, raw_labels.y, raw_labels.x], dims=[\"band\", \"y\", \"x\"])\n",
    "\n",
    "final_label_ERA5_processed = xr.Dataset({\n",
    "    '__xarray_dataarray_variable__': new_da\n",
    "    }, coords={\n",
    "    'band': new_bands,\n",
    "    'x': raw_labels.x,\n",
    "    'y': raw_labels.y\n",
    "    }) # the new label xarray dataset\n",
    "\n",
    "final_label_ERA5_processed = final_label_ERA5_processed.rename({'band': 'time'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label_ERA5_processed.to_netcdf(f'{save_path}/final_label_Full_ERA5.nc', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini datacubes data processing\n",
    "* The resolution is the same as the full resolution \n",
    "* Each data cube bounding box correspond to one point at ERA5 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for latitude in ERA5.latitude.values:\n",
    "    for longitude in ERA5.longitude.values:\n",
    "        bbox=(longitude-.125, latitude-.125, longitude+.125, latitude+.125)\n",
    "        if(bbox[2]>min(final_label.x.values)) & (bbox[0]<max(final_label.x.values)) & (bbox[3]>min(final_label.y.values)) & (bbox[1]<max(final_label.y.values)):\n",
    "            yIndex = (final_label.y>bbox[1]) & (final_label.y<bbox[3])\n",
    "            xIndex = (final_label.x>bbox[0]) & (final_label.x<bbox[2])\n",
    "            tmp = final_label.sel(y=slice(bbox[3],bbox[1]), x=slice(bbox[0],bbox[2]))\n",
    "            tmp.to_netcdf(f\"{save_path}/smallbox/label/label_{str(bbox)}.nc\", engine='h5netcdf')\n",
    "            tmp = processed_static.sel(y=slice(bbox[3],bbox[1]), x=slice(bbox[0],bbox[2]))\n",
    "            tmp.to_netcdf(f\"{save_path}/smallbox/static/static_{str(bbox)}.nc\", engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_label = xr.open_dataset(f'{save_path}/final_label_Full_Rez.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(final_label.__xarray_dataarray_variable__.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 124472 / 413927287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clef_aso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
